{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(888)\n",
    "import os\n",
    "import numpy as np\n",
    "from time import strftime\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_PATH = 'MNIST/digit_xtrain.csv'\n",
    "X_TEST_PATH = 'MNIST/digit_xtest.csv'\n",
    "Y_TRAIN_PATH = 'MNIST/digit_ytrain.csv'\n",
    "Y_TEST_PATH = 'MNIST/digit_ytest.csv'\n",
    "\n",
    "LOGGING_PATH = 'tensorboard_mnist_digit_logs/'\n",
    "\n",
    "NR_CLASSES = 10\n",
    "VALIDATION_SIZE = 10000\n",
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "CHANNELS = 1\n",
    "TOTAL_INPUTS = IMAGE_WIDTH*IMAGE_HEIGHT*CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.loadtxt(Y_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_train_all = np.loadtxt(X_TRAIN_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 736 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_test = np.loadtxt(X_TEST_PATH, delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scale\n",
    "x_train_all, x_test = x_train_all / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert target values to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = y_train_all[:5]\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(10)[values]\n",
    "np.eye(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(10)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = np.eye(NR_CLASSES)[y_train_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.eye(NR_CLASSES)[y_test]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create validation dataset from training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:** Split the training dataset into a smaller training dataset and a validation dataset for the features and the labels. Create four arrays: ```x_val```, ```y_val```, ```x_train```, and ```y_train``` from ```x_train_all``` and ```y_train_all```. Use the validation size of 10,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train_all[:VALIDATION_SIZE]\n",
    "y_val = y_train_all[:VALIDATION_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_all[VALIDATION_SIZE:]\n",
    "y_train = y_train_all[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Setup Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NR_CLASSES = 10\n",
    "# VALIDATION_SIZE = 10000\n",
    "# IMAGE_WIDTH = 28\n",
    "# IMAGE_HEIGHT = 28\n",
    "# CHANNELS = 1\n",
    "# TOTAL_INPUTS = IMAGE_WIDTH*IMAGE_HEIGHT*CHANNELS\n",
    "\n",
    "# X = tf.compat.v1.placeholder(tf.float32, shape=[None, TOTAL_INPUTS], name='X')\n",
    "# Y = tf.compat.v1.placeholder(tf.float32, shape=[None, NR_CLASSES], name='labels')\n",
    "\n",
    "# nr_epochs = 50\n",
    "# learning_rate = 1e-3\n",
    "\n",
    "# n_hidden1 = 512\n",
    "# n_hidden2 = 64\n",
    "\n",
    "\n",
    "# def setup_layer(input_tensor, weight_dim, bias_dim, name):\n",
    "    \n",
    "#     with tf.name_scope(name):\n",
    "#         initial_w = tf.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\n",
    "#         w = tf.Variable(initial_value=initial_w, name='W')\n",
    "\n",
    "#         initial_b = tf.constant(value=0.0, shape=bias_dim)\n",
    "#         b = tf.Variable(initial_value=initial_b, name='B')\n",
    "\n",
    "#         layer_in = tf.matmul(input_tensor, w) + b\n",
    "        \n",
    "#         if name=='out':\n",
    "#             layer_out = tf.nn.softmax(layer_in)\n",
    "#         else:\n",
    "#             layer_out = tf.nn.relu(layer_in)\n",
    "        \n",
    "#         tf.summary.histogram('weights', w)\n",
    "#         tf.summary.histogram('biases', b)\n",
    "        \n",
    "#         return layer_out\n",
    "    \n",
    "    \n",
    "# layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1], \n",
    "#                       bias_dim=[n_hidden1], name='layer_1')\n",
    "\n",
    "# layer_drop = tf.nn.dropout(layer_1, keep_prob=0.8, name='dropout_layer')\n",
    "\n",
    "# layer_2 = setup_layer(layer_drop, weight_dim=[n_hidden1, n_hidden2], \n",
    "#                       bias_dim=[n_hidden2], name='layer_2')\n",
    "\n",
    "# output = setup_layer(layer_2, weight_dim=[n_hidden2, NR_CLASSES], \n",
    "#                       bias_dim=[NR_CLASSES], name='out')\n",
    "\n",
    "# model_name = f'{n_hidden1}-DO-{n_hidden2} LR{learning_rate} E{nr_epochs}'\n",
    "\n",
    "# # Folder for Tensorboard\n",
    "\n",
    "# folder_name = f'{model_name} at {strftime(\"%H:%M\")}'\n",
    "# directory = os.path.join(LOGGING_PATH, folder_name)\n",
    "\n",
    "# try:\n",
    "#     os.makedirs(directory)\n",
    "# except OSError as exception:\n",
    "#     print(exception.strerror)\n",
    "# else:\n",
    "#     print('Successfully created directories!')\n",
    "    \n",
    "\n",
    "# with tf.name_scope('loss_calc'):\n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=output))\n",
    "    \n",
    "# with tf.name_scope('optimizer'):\n",
    "#     optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "#     train_step = optimizer.minimize(loss)    \n",
    "    \n",
    "# with tf.name_scope('accuracy_calc'):\n",
    "#     correct_pred = tf.equal(tf.argmax(output, axis=1), tf.argmax(Y, axis=1))\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "# with tf.name_scope('performance'):\n",
    "#     tf.summary.scalar('accuracy', accuracy)\n",
    "#     tf.summary.scalar('cost', loss)\n",
    "    \n",
    "# with tf.name_scope('show_image'):\n",
    "#     x_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "#     tf.summary.image('image_input', x_image, max_outputs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture\n",
    "\n",
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model without dropout\n",
    "# layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1], \n",
    "#                       bias_dim=[n_hidden1], name='layer_1')\n",
    "\n",
    "# layer_2 = setup_layer(layer_1, weight_dim=[n_hidden1, n_hidden2], \n",
    "#                       bias_dim=[n_hidden2], name='layer_2')\n",
    "\n",
    "# output = setup_layer(layer_2, weight_dim=[n_hidden2, NR_CLASSES], \n",
    "#                       bias_dim=[NR_CLASSES], name='out')\n",
    "\n",
    "# model_name = f'{n_hidden1}-{n_hidden2} LR{learning_rate} E{nr_epochs}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_1 = setup_layer(X, weight_dim=[TOTAL_INPUTS, n_hidden1], \n",
    "#                       bias_dim=[n_hidden1], name='layer_1')\n",
    "\n",
    "# layer_drop = tf.nn.dropout(layer_1, keep_prob=0.8, name='dropout_layer')\n",
    "\n",
    "# layer_2 = setup_layer(layer_drop, weight_dim=[n_hidden1, n_hidden2], \n",
    "#                       bias_dim=[n_hidden2], name='layer_2')\n",
    "\n",
    "# output = setup_layer(layer_2, weight_dim=[n_hidden2, NR_CLASSES], \n",
    "#                       bias_dim=[NR_CLASSES], name='out')\n",
    "\n",
    "# model_name = f'{n_hidden1}-DO-{n_hidden2} LR{learning_rate} E{nr_epochs}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    # First hidden layer with 512 units and ReLU activation\n",
    "    Dense(n_hidden1, activation='relu', input_shape=(TOTAL_INPUTS,), name='layer_1'),\n",
    "    \n",
    "    # Dropout layer with 20% drop rate\n",
    "    Dropout(rate=0.2, name='dropout_layer'),\n",
    "    \n",
    "    # Second hidden layer with 64 units and ReLU activation\n",
    "    Dense(n_hidden2, activation='relu', name='layer_2'),\n",
    "    \n",
    "    # Output layer with NR_CLASSES units and softmax activation\n",
    "    Dense(NR_CLASSES, activation='softmax', name='out')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder for Tensorboard\n",
    "\n",
    "# folder_name = f'{model_name} at {strftime(\"%H:%M\")}'\n",
    "# directory = os.path.join(LOGGING_PATH, folder_name)\n",
    "\n",
    "# try:\n",
    "#     os.makedirs(directory)\n",
    "# except OSError as exception:\n",
    "#     print(exception.strerror)\n",
    "# else:\n",
    "#     print('Successfully created directories!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'{n_hidden1}-DO-{n_hidden2} LR{learning_rate} E{nr_epochs}'\n",
    "folder_name = f'{model_name} at {strftime(\"%H_%M\")}'\n",
    "directory = os.path.join(LOGGING_PATH, folder_name)\n",
    "os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, Optimisation & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining Loss Function\n",
    "# with tf.name_scope('loss_calc'):\n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=output))\n",
    "\n",
    "# # Defining Optimizer\n",
    "# with tf.name_scope('optimizer'):\n",
    "#     optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "#     train_step = optimizer.minimize(loss)\n",
    "    \n",
    "# # Accuracy Metric\n",
    "# with tf.name_scope('accuracy_calc'):\n",
    "#     correct_pred = tf.equal(tf.argmax(output, axis=1), tf.argmax(Y, axis=1))\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "# with tf.name_scope('performance'):\n",
    "#     tf.summary.scalar('accuracy', accuracy)\n",
    "#     tf.summary.scalar('cost', loss)\n",
    "    \n",
    "# # Check Input Images in Tensorboard\n",
    "# with tf.name_scope('show_image'):\n",
    "#     x_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "#     tf.summary.image('image_input', x_image, max_outputs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.Session()\n",
    "\n",
    "# #Setup Filewriter and Merge Summaries\n",
    "# merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# train_writer = tf.summary.FileWriter(directory + '/train')\n",
    "# train_writer.add_graph(sess.graph)\n",
    "\n",
    "# validation_writer = tf.summary.FileWriter(directory + '/validation')\n",
    "\n",
    "# #initialise the variables\n",
    "# init = tf.global_variables_initializer()\n",
    "# sess.run(init)\n",
    "\n",
    "# #batching the data\n",
    "# size_of_batch = 1000\n",
    "\n",
    "# num_examples = y_train.shape[0]\n",
    "# nr_iterations = int(num_examples/size_of_batch)\n",
    "\n",
    "# index_in_epoch = 0\n",
    "\n",
    "\n",
    "# def next_batch(batch_size, data, labels):\n",
    "    \n",
    "#     global num_examples\n",
    "#     global index_in_epoch\n",
    "    \n",
    "#     start = index_in_epoch\n",
    "#     index_in_epoch += batch_size\n",
    "    \n",
    "#     if index_in_epoch > num_examples:\n",
    "#         start = 0\n",
    "#         index_in_epoch = batch_size\n",
    "    \n",
    "#     end = index_in_epoch\n",
    "    \n",
    "#     return data[start:end], labels[start:end]\n",
    "\n",
    "# # training loop\n",
    "# for epoch in range(nr_epochs):\n",
    "    \n",
    "#     # ============= Training Dataset =========\n",
    "#     for i in range(nr_iterations):\n",
    "        \n",
    "#         batch_x, batch_y = next_batch(batch_size=size_of_batch, data=x_train, labels=y_train)\n",
    "        \n",
    "#         feed_dictionary = {X:batch_x, Y:batch_y}\n",
    "        \n",
    "#         sess.run(train_step, feed_dict=feed_dictionary)\n",
    "        \n",
    "    \n",
    "#     s, batch_accuracy = sess.run(fetches=[merged_summary, accuracy], feed_dict=feed_dictionary)\n",
    "        \n",
    "#     train_writer.add_summary(s, epoch)\n",
    "    \n",
    "#     print(f'Epoch {epoch} \\t| Training Accuracy = {batch_accuracy}')\n",
    "    \n",
    "#     # ================== Validation ======================\n",
    "    \n",
    "#     summary = sess.run(fetches=merged_summary, feed_dict={X:x_val, Y:y_val})\n",
    "#     validation_writer.add_summary(summary, epoch)\n",
    "\n",
    "# print('Done training!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 [==============================] - 8s 90ms/step - loss: 0.6251 - accuracy: 0.8206 - val_loss: 0.2526 - val_accuracy: 0.9286\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 4s 72ms/step - loss: 0.2286 - accuracy: 0.9324 - val_loss: 0.1782 - val_accuracy: 0.9484\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1646 - accuracy: 0.9520 - val_loss: 0.1435 - val_accuracy: 0.9591\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.1297 - accuracy: 0.9626 - val_loss: 0.1232 - val_accuracy: 0.9640\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.1059 - accuracy: 0.9691 - val_loss: 0.1053 - val_accuracy: 0.9694\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.0885 - accuracy: 0.9734 - val_loss: 0.0994 - val_accuracy: 0.9708\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.0751 - accuracy: 0.9777 - val_loss: 0.0885 - val_accuracy: 0.9730\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.0650 - accuracy: 0.9806 - val_loss: 0.0826 - val_accuracy: 0.9746\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.0548 - accuracy: 0.9837 - val_loss: 0.0800 - val_accuracy: 0.9766\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.0485 - accuracy: 0.9858 - val_loss: 0.0775 - val_accuracy: 0.9763\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.0422 - accuracy: 0.9875 - val_loss: 0.0764 - val_accuracy: 0.9771\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.0372 - accuracy: 0.9895 - val_loss: 0.0716 - val_accuracy: 0.9791\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.0327 - accuracy: 0.9903 - val_loss: 0.0687 - val_accuracy: 0.9800\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.0286 - accuracy: 0.9915 - val_loss: 0.0676 - val_accuracy: 0.9800\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.0248 - accuracy: 0.9931 - val_loss: 0.0738 - val_accuracy: 0.9779\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.0240 - accuracy: 0.9928 - val_loss: 0.0690 - val_accuracy: 0.9805\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.0213 - accuracy: 0.9943 - val_loss: 0.0684 - val_accuracy: 0.9797\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.0182 - accuracy: 0.9953 - val_loss: 0.0665 - val_accuracy: 0.9816\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.0160 - accuracy: 0.9957 - val_loss: 0.0684 - val_accuracy: 0.9806\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.0152 - accuracy: 0.9960 - val_loss: 0.0680 - val_accuracy: 0.9818\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.0136 - accuracy: 0.9965 - val_loss: 0.0684 - val_accuracy: 0.9814\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.0129 - accuracy: 0.9966 - val_loss: 0.0730 - val_accuracy: 0.9796\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.0114 - accuracy: 0.9972 - val_loss: 0.0673 - val_accuracy: 0.9823\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.0103 - accuracy: 0.9976 - val_loss: 0.0687 - val_accuracy: 0.9821\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.0099 - accuracy: 0.9974 - val_loss: 0.0702 - val_accuracy: 0.9813\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.0092 - accuracy: 0.9978 - val_loss: 0.0702 - val_accuracy: 0.9816\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.0087 - accuracy: 0.9979 - val_loss: 0.0724 - val_accuracy: 0.9809\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.0074 - accuracy: 0.9983 - val_loss: 0.0700 - val_accuracy: 0.9827\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.0072 - accuracy: 0.9983 - val_loss: 0.0686 - val_accuracy: 0.9818\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.0062 - accuracy: 0.9986 - val_loss: 0.0702 - val_accuracy: 0.9811\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.0057 - accuracy: 0.9989 - val_loss: 0.0697 - val_accuracy: 0.9818\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 0.0700 - val_accuracy: 0.9820\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.0057 - accuracy: 0.9985 - val_loss: 0.0729 - val_accuracy: 0.9822\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 0.0717 - val_accuracy: 0.9813\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.0057 - accuracy: 0.9987 - val_loss: 0.0719 - val_accuracy: 0.9831\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.0737 - val_accuracy: 0.9822\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0731 - val_accuracy: 0.9821\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0045 - accuracy: 0.9989 - val_loss: 0.0736 - val_accuracy: 0.9822\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.0773 - val_accuracy: 0.9821\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.0052 - accuracy: 0.9986 - val_loss: 0.0754 - val_accuracy: 0.9822\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.0793 - val_accuracy: 0.9819\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 2s 45ms/step - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.0766 - val_accuracy: 0.9825\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.0812 - val_accuracy: 0.9812\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.0728 - val_accuracy: 0.9836\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 2s 49ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.0755 - val_accuracy: 0.9831\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 2s 48ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0766 - val_accuracy: 0.9829\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.0799 - val_accuracy: 0.9823\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 2s 47ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.0796 - val_accuracy: 0.9817\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 2s 46ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.0758 - val_accuracy: 0.9823\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 2s 51ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0743 - val_accuracy: 0.9828\n",
      "Done training!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# Define TensorBoard callback\n",
    "tensorboard_callback = TensorBoard(log_dir=directory, histogram_freq=1)\n",
    "\n",
    "# Defining the batch size\n",
    "size_of_batch = 1000\n",
    "\n",
    "# Fit the model using the training data\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=size_of_batch,\n",
    "    epochs=nr_epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n",
    "print('Done training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('MNIST/test_img.png')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = Image.open('MNIST/test_img.png')\n",
    "# bw = img.convert('L')\n",
    "# img_array = np.invert(bw)\n",
    "# test_img = img_array.ravel()\n",
    "# prediction = sess.run(fetches=tf.argmax(output, axis=1), feed_dict={X:[test_img]})\n",
    "# print(f'Prediction for test image is {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "Prediction for test image is [2]\n"
     ]
    }
   ],
   "source": [
    "img = Image.open('MNIST/test_img.png')\n",
    "bw = img.convert('L')\n",
    "img_array = np.invert(bw)\n",
    "test_img = img_array.ravel().reshape(1, -1) # Make sure to reshape the image to match the input shape\n",
    "prediction = model.predict(test_img)\n",
    "predicted_class = np.argmax(prediction, axis=1)\n",
    "\n",
    "print(f'Prediction for test image is {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-71800705548d497b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-71800705548d497b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:** Calculate the accuracy over the test dataset (```x_test``` and ```y_test```). Use your knowledge of running a session to get the accuracy. Display the accuracy as a percentage rounded to two decimal numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 2ms/step - loss: 0.0743 - accuracy: 0.9820\n",
      "Accuracy on test set is 98.20%\n"
     ]
    }
   ],
   "source": [
    "# test_accuracy = sess.run(fetches=accuracy, feed_dict={X:x_test, Y:y_test})\n",
    "# print(f'Accuracy on test set is {test_accuracy:0.2%}')\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'Accuracy on test set is {test_accuracy:0.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reset for the Next Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_writer.close()\n",
    "# validation_writer.close()\n",
    "# sess.close()\n",
    "# tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Code for 1st Part of Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('hidden_1'):\n",
    "\n",
    "#     initial_w1 = tf.truncated_normal(shape=[TOTAL_INPUTS, n_hidden1], stddev=0.1, seed=42)\n",
    "#     w1 = tf.Variable(initial_value=initial_w1, name='w1')\n",
    "\n",
    "#     initial_b1 = tf.constant(value=0.0, shape=[n_hidden1])\n",
    "#     b1 = tf.Variable(initial_value=initial_b1, name='b1')\n",
    "\n",
    "#     layer1_in = tf.matmul(X, w1) + b1\n",
    "\n",
    "#     layer1_out = tf.nn.relu(layer1_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:** Set up the second hindden layer. This layer has 64 neurons and needs to work off the output of the first hidden layer (see above). Then setup the output layer. Remember, the output layer will use the softmax activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('hidden_2'):\n",
    "\n",
    "#     initial_w2 = tf.truncated_normal(shape=[n_hidden1, n_hidden2], stddev=0.1, seed=42)\n",
    "#     w2 = tf.Variable(initial_value=initial_w2, name='w2')\n",
    "\n",
    "#     initial_b2 = tf.constant(value=0.0, shape=[n_hidden2])\n",
    "#     b2 = tf.Variable(initial_value=initial_b2, name='b2')\n",
    "\n",
    "#     layer2_in = tf.matmul(layer1_out, w2) + b2\n",
    "#     layer2_out = tf.nn.relu(layer2_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('output_layer'):\n",
    "\n",
    "#     initial_w3 = tf.truncated_normal(shape=[n_hidden2, NR_CLASSES], stddev=0.1, seed=42)\n",
    "#     w3 = tf.Variable(initial_value=initial_w3, name='w3')\n",
    "\n",
    "#     initial_b3 = tf.constant(value=0.0, shape=[NR_CLASSES])\n",
    "#     b3 = tf.Variable(initial_value=initial_b3, name='b3')\n",
    "\n",
    "#     layer3_in = tf.matmul(layer2_out, w3) + b3\n",
    "#     output = tf.nn.softmax(layer3_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b3.eval(sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
